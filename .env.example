# Copy this file to `.env` (it is gitignored) and edit as needed.
#
# Then start the gateway with:
#   ./scripts/serve.sh

# Minimal config: use a preset to avoid tuning lots of env vars.
# CODEX_PRESET=codex-fast

# Optional auth (if set, clients must send: Authorization: Bearer <token>)
CODEX_GATEWAY_TOKEN=devtoken

# Logging
# - summary: one line per request/response
# - qa: show last USER message + assistant output (recommended)
# - full: show full prompt + full assistant output
CODEX_LOG_MODE=qa
# Optional: log raw CLI/backend events (very noisy)
# CODEX_LOG_EVENTS=1
# Truncate long log blocks
CODEX_LOG_MAX_CHARS=4000

# For clients that parse a single do(...)/finish(...) action (e.g. Open-AutoGLM)
CODEX_STRIP_ANSWER_TAGS=1

# Large prompts / screenshots can make codex JSONL events exceed default asyncio pipe limits
CODEX_SUBPROCESS_STREAM_LIMIT=16777216

# Prevent OpenAI SDK streaming read timeouts while Codex is thinking
CODEX_SSE_KEEPALIVE_SECONDS=2

# Prefer native vision over Codex's MCP-based image viewing tool (reduces MCP tool calls)
CODEX_DISABLE_VIEW_IMAGE_TOOL=1

# Optional: use Codex backend `/responses` API for all Codex requests (true token streaming).
# Vision requests auto-use `/responses` even if this is unset.
CODEX_USE_CODEX_RESPONSES_API=1
# CODEX_CODEX_BASE_URL=https://chatgpt.com/backend-api/codex
# CODEX_CODEX_VERSION=0.21.0
# CODEX_CODEX_USER_AGENT="codex_cli_rs/0.73.0 (Mac OS 26.0.1; arm64) Apple_Terminal/464"

# Default model and reasoning
CODEX_MODEL=gpt-5.2
CODEX_MODEL_REASONING_EFFORT=low
# Optional: force reasoning effort for all requests (overrides client-provided reasoning)
# CODEX_FORCE_REASONING_EFFORT=low

# Optional: accept alternative model ids from clients by mapping them to a real Codex model
# CODEX_MODEL_ALIASES={"autoglm-phone":"gpt-5.2"}
# Optional: customize GET /v1/models output
# CODEX_ADVERTISED_MODELS=gpt-5.2,gpt-5-codex

# Optional performance: point Codex at a minimal HOME to avoid starting MCP servers per request
# CODEX_CLI_HOME=/absolute/path/to/.codex-gateway-home

# Optional multi-provider (requires the corresponding CLIs installed & authenticated)
# Provider routing (who decides which agent):
# - auto (default): clients can choose by prefixing `model` (cursor:/claude:/gemini:)
# - codex|cursor-agent|claude|gemini: force a single provider for the whole gateway
# Tip: if you set `CODEX_PRESET=...`, you usually don't need to set `CODEX_PROVIDER` manually.
# CODEX_PROVIDER=codex
# If 1, always allow request-side provider prefixes to override CODEX_PROVIDER.
CODEX_ALLOW_CLIENT_PROVIDER_OVERRIDE=0
# If 1, allow client-sent `model` to override the provider-specific model selection.
# If 0, the gateway always uses its configured defaults (e.g. CURSOR_AGENT_MODEL=auto).
CODEX_ALLOW_CLIENT_MODEL_OVERRIDE=0

# Cursor Agent:
# CURSOR_AGENT_MODEL=auto
# CURSOR_AGENT_WORKSPACE=/path/to/empty/workspace
# CURSOR_AGENT_DISABLE_INDEXING=1
# CURSOR_AGENT_EXTRA_ARGS="--endpoint https://api2.cursor.sh --http-version 2"
# CURSOR_AGENT_API_KEY=...   # or set CURSOR_API_KEY
#
# Claude Code:
# CLAUDE_MODEL=sonnet
# Claude direct OAuth (Anthropic HTTP API + SSE). Requires an OAuth cache file.
# CLAUDE_USE_OAUTH_API=1
# CLAUDE_OAUTH_CREDS_PATH=~/.claude/oauth_creds.json
# CLAUDE_OAUTH_BASE_URL=https://console.anthropic.com
# CLAUDE_API_BASE_URL=https://api.anthropic.com
# OAuth bootstrap helper:
#   uv run python -m codex_gateway.claude_oauth_login
#
# Gemini CLI:
# GEMINI_MODEL=auto
# Gemini CloudCode (Gemini CLI backend, direct HTTP; no API key needed if Gemini CLI is already logged in)
# GEMINI_USE_CLOUDCODE_API=1
# GEMINI_OAUTH_CREDS_PATH=~/.gemini/oauth_creds.json
# GEMINI_PROJECT_ID=
# If token refresh fails and auto-detection can't find the CLI OAuth client in your `gemini` binary:
# GEMINI_OAUTH_CLIENT_ID=...
# GEMINI_OAUTH_CLIENT_SECRET=...
